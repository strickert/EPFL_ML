{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcp6PULBCqss"
   },
   "source": [
    "# 5. Capstone Project: Machine Learning Models IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxLOusgfEwks"
   },
   "source": [
    "***\n",
    "\n",
    "![headerall](./images/headers/header_all.jpg)\n",
    "\n",
    "##  Goals\n",
    "\n",
    "### Project:\n",
    "In this work, we will first analyze where and when traffic congestion is highest and lowest in New York State. We will then build different machine learning models capable of predicting cab travel times in and around New York City using only variables that can be easily obtained from a smartphone app or a website. We will then compare their performance and explore the possibility of using additional variables such as weather forecasts and holidays to improve the predictive performance of the models.\n",
    "\n",
    "### Section:\n",
    "In this section, we will use the knowledge gained during the exploratory data analysis to perform the final feature transformation. Next, we will create and compare the performance of three machine learning models based on linear regression, support vector machine, and a gradient enhanced decision tree. Hyperparameters will be optimized for each model to achieve the best possible performance.\n",
    "\n",
    "## Data\n",
    "### External Datasets:\n",
    "- Weather Forecast: The 2018 NYC weather forecast was collected from the [National Weather Service Forecast Office](https://w2.weather.gov/climate/index.php?wfo=okx) website. Daily measurements were taken from January to December 2018 in Central Park. These measures are given in imperial units and include daily minimum and maximum temperatures, precipitations, snowfall, and snow depth.\n",
    "\n",
    "- Holidays: The 2018 NYC holidays list was collected from the [Office Holiday](https://www.officeholidays.com/countries/usa/new-york/2021) website. The dataset contains the name, date, and type of holidays for New York.\n",
    "\n",
    "- Taxi Zones: The NYC Taxi Zones dataset was collected from the [NYC Open Data](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc) website. It contains the pickup and drop-off zones (Location IDs) for the Yellow, Green, and FHV Trip Records. The taxi zones are based on the NYC Department of City Planning’s Neighborhood.\n",
    "\n",
    "### Primary Datasets:\n",
    "\n",
    "- Taxi Trips: The 2018 NYC Taxi Trip dataset was collected from the [Google Big Query](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?project=jovial-monument-300209&folder=&organizationId=) platform. The dataset contains more than 100'000'000 Yellow Taxi Trip records for 2018 and contains an extensive amount of variables including the pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8tvC_V2FEj5"
   },
   "source": [
    "***\n",
    "## Table of Content:\n",
    "    1. Data Preparation\n",
    "        1.1 External Datasets\n",
    "            1.1.1 Weather Forecast Dataset\n",
    "            1.1.2 Holidays Dataset\n",
    "            1.1.3 Taxi Zones Dataset\n",
    "        1.2 Primary Dataset\n",
    "            1.2.1 Taxi Trips Dataset\n",
    "            1.2.2 Taxi Trips Subset\n",
    "    2. Exploratory Data Analysis\n",
    "        2.1 Primary Dataset\n",
    "            2.1.1 Temporal Analysis\n",
    "            2.1.2 Spatio-Temporal Analysis\n",
    "        2.2 External Datasets\n",
    "            2.2.1 Temporal Analysis of Weather Data\n",
    "            2.2.2 Temporal Analysis of Holidays Data\n",
    "        2.3 Combined Dataset\n",
    "            2.3.1 Overall Features Correlation\n",
    "    3. Machine Learning Models\n",
    "        3.1 Data Preparation\n",
    "        3.2 Baselines\n",
    "        3.2 Model Training\n",
    "            3.2.1 Linear Regressions\n",
    "            3.2.2 Support Vector Machine\n",
    "            3.2.3 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1WFf7qqEi03",
    "tags": []
   },
   "source": [
    "***\n",
    "## Python Libraries and Magic commands Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data processing libraris gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization librairies\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import median_absolute_error as MAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up magic commands\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: (824654, 33)\n",
      "y_tr: (824654,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the train dataset\n",
    "train_df = pd.read_pickle(r'data/processed/train.pickle')\n",
    "\n",
    "# Get the independant variables from the train dataset\n",
    "X_tr = train_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the train dataset\n",
    "y_tr = train_df[\"trip_duration\"]\n",
    "\n",
    "print('X_tr:', X_tr.shape)\n",
    "print('y_tr:', y_tr.shape, y_tr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_te: (206156, 33)\n",
      "y_te: (206156,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the test dataset\n",
    "test_df = pd.read_pickle(r'data/processed/test.pickle')\n",
    "\n",
    "# Get the independant variables from the test dataset\n",
    "X_te = test_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the test dataset\n",
    "y_te = test_df[\"trip_duration\"]\n",
    "\n",
    "print('X_te:', X_te.shape)\n",
    "print('y_te:', y_te.shape, y_te.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Functions Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that performs preprocessing steps to the selected dataset\n",
    "def preprocess(data, categorical_cols, continuous_cols, transform_cols, polynome_deg=1):\n",
    "\n",
    "    # Create a copy of the data frame\n",
    "    df = data.copy()\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "    # Log-transform numerical variables\n",
    "    for col in transform_cols:\n",
    "        df[col] = np.log(df[col])\n",
    "    \n",
    "    # Add polynomial features\n",
    "    for col in continuous_cols:\n",
    "        if polynome_deg > 1:\n",
    "            for poly in range(polynome_deg + 1):\n",
    "                df[\"{}**{}\".format(col, poly)] = df[col] ** poly\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Variable Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of categorical variables\n",
    "categorical_cols = [\n",
    "    \"pickup_month\",\n",
    "    \"pickup_week\",\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekday_type\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_hour_type\",\n",
    "    \"wf_avg_temp_lvl\",\n",
    "    \"wf_prec_lvl\",\n",
    "    \"wf_new_snow_lvl\",\n",
    "    \"wf_snow_depth_lvl\",\n",
    "    \"holiday_type\",\n",
    "    \"holiday\",\n",
    "    \"trip_within_borough\",\n",
    "    \"tolls_amount_lvl\",\n",
    "]\n",
    "\n",
    "# Define a list of continuous variables\n",
    "continuous_cols = [\n",
    "    \"trip_distance\",\n",
    "    \"tolls_amount\",\n",
    "    \"wf_avg_temp\",\n",
    "    \"wf_prec\",\n",
    "    \"wf_new_snow\",\n",
    "    \"wf_snow_depth\",\n",
    "    \"pickup_zone_latitude\",\n",
    "    \"pickup_zone_longitude\",\n",
    "    \"pickup_borough_latitude\",\n",
    "    \"pickup_borough_longitude\",\n",
    "    \"dropoff_zone_latitude\",\n",
    "    \"dropoff_zone_longitude\",\n",
    "    \"dropoff_borough_latitude\",\n",
    "    \"dropoff_borough_longitude\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# 3. Machine Learning Models\n",
    "## 3.3 Machine Learning Models: Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get id column names from the train dataset\n",
    "id_cols = [c for c in train_df.columns if \"_id\" in c]\n",
    "\n",
    "# Remove ID features in the train dataset\n",
    "train_df.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Remove ID features in the test dataset\n",
    "test_df.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the train dataset\n",
    "train_df.drop(\"pickup_yearday\", axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the test dataset\n",
    "test_df.drop(\"pickup_yearday\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.3.1 Model Training: Decision Trees\n",
    "\n",
    "## Goals:\n",
    "\n",
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10,  20,  30,  40,  50,  60,  70,  80,  90, 100])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10, 110, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'standardscaler', 'randomforestregressor', 'standardscaler__copy', 'standardscaler__with_mean', 'standardscaler__with_std', 'randomforestregressor__bootstrap', 'randomforestregressor__ccp_alpha', 'randomforestregressor__criterion', 'randomforestregressor__max_depth', 'randomforestregressor__max_features', 'randomforestregressor__max_leaf_nodes', 'randomforestregressor__max_samples', 'randomforestregressor__min_impurity_decrease', 'randomforestregressor__min_impurity_split', 'randomforestregressor__min_samples_leaf', 'randomforestregressor__min_samples_split', 'randomforestregressor__min_weight_fraction_leaf', 'randomforestregressor__n_estimators', 'randomforestregressor__n_jobs', 'randomforestregressor__oob_score', 'randomforestregressor__random_state', 'randomforestregressor__verbose', 'randomforestregressor__warm_start'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "The MSE of the linear regression model is: 18\n",
      "The MAE of the linear regression model is: 2\n",
      "\n",
      " The best parameters across all searched params:\n",
      " {'randomforestregressor__max_depth': 40}\n"
     ]
    }
   ],
   "source": [
    "sr_model = make_pipeline(StandardScaler(),\n",
    "                RandomForestRegressor(random_state=0))\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "sr_model_params = {\n",
    "    \"randomforestregressor__max_depth\": np.arange(10, 110, 10)\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "sr_model_gs = GridSearchCV(\n",
    "    sr_model,\n",
    "    sr_model_params,\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "sr_model_gs.fit(X_tr[:100000], y_tr[:100000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "sr_y_pred = sr_model_gs.predict(X_te)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, sr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, sr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params:\\n\",sr_model_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/rwdq78h925zdjg_0ccng3f4r0000gn/T/ipykernel_40814/3873123768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a subset of the train matrix without holiday and weather forecastdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_tr_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"wf\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"holiday\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create a subset of the test matrix without holiday and weather forecast data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_te_sub1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"wf\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"holiday\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a subset of the train matrix without holiday and weather forecastdata\n",
    "X_tr_sub1 = X_tr.drop(columns=[col for col in X_tr.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday and weather forecast data\n",
    "X_te_sub1 = X_te.drop(columns=[col for col in X_te.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub1.shape)\n",
    "print(\"X_te:\", X_te_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "The MSE of the linear regression model is: 18\n",
      "The MAE of the linear regression model is: 2\n",
      "\n",
      " The best parameters across all searched params:\n",
      " {'randomforestregressor__max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "sr_model = make_pipeline(StandardScaler(),\n",
    "                RandomForestRegressor(random_state=0))\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "sr_model_params = {\n",
    "    \"randomforestregressor__max_depth\": np.arange(10, 40, 10)\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "sr_model_gs = GridSearchCV(\n",
    "    sr_model,\n",
    "    sr_model_params,\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "sr_model_gs.fit(X_tr_sub1[:100000], y_tr[:100000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "sr_y_pred = sr_model_gs.predict(X_te_sub1)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, sr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, sr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params:\\n\",sr_model_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8069281108751656"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_model_gs.score(X_te_sub1, y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr_sub1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/rwdq78h925zdjg_0ccng3f4r0000gn/T/ipykernel_40814/2160069190.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_sub1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_te_sub1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_df_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_tr_sub1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "dtrain = xgb.DMatrix(X_tr_sub1, label=train_y)\n",
    "dvalid = xgb.DMatrix(X_te_sub1, label=val_y)\n",
    "dtest = xgb.DMatrix(new_df_test.values)\n",
    "watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "# Tune these params, see https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "xgb_pars = {'min_child_weight': 100, 'eta': 0.1, 'colsample_bytree': 0.7, 'max_depth': 15,\n",
    "            'subsample': 0.8, 'lambda': 1., 'nthread': -1, 'booster' : 'gbtree', 'silent': 1,\n",
    "            'eval_metric': 'mse', 'objective': 'reg:linear'}\n",
    "\n",
    "model_xgb = xgb.train(xgb_pars, dtrain, 500, watchlist, early_stopping_rounds=50,\n",
    "                  maximize=False, verbose_eval=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DecisionTrees' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/rwdq78h925zdjg_0ccng3f4r0000gn/T/ipykernel_40399/649991972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     TransformedTargetRegressor(\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mregressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     ),\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DecisionTrees' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=DecisionTrees(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the train dataset\n",
    "lr_model.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict the target variable of the test dataset\n",
    "lr_y_pred = lr_model.predict(X_te)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/rwdq78h925zdjg_0ccng3f4r0000gn/T/ipykernel_40399/3722387661.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msr_y_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'score'"
     ]
    }
   ],
   "source": [
    "sr_y_pred.score(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p.shape)\n",
    "print(\"X_te:\", X_te_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "lr_model.fit(X_tr_p, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression: testing different linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p0 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p0 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p.shape)\n",
    "print(\"X_te:\", X_te_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessedtrain dataset\n",
    "lr_model.fit(X_tr_p0, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p0)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to Ridge regression model\n",
    "rr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(regressor=Ridge(), func=np.log, inverse_func=np.exp),\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "rr_model_params = {\n",
    "    \"transformedtargetregressor__regressor__alpha\": np.logspace(-1, 4, num=10)\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "rr_model_gs = GridSearchCV(\n",
    "    rr_model,\n",
    "    rr_model_params,\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rr_model_gs.fit(X_tr_p0, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rr_y_pred = rr_model_gs.predict(X_te_p0)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, rr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, rr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params:\\n\",rr_model_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to Ridge regression model\n",
    "sr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(regressor=SGDRegressor(random_state=0), func=np.log, inverse_func=np.exp),\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "sr_model_params = {\n",
    "    \"transformedtargetregressor__regressor__alpha\": np.logspace(-3,-7, num= 5),\n",
    "    \"transformedtargetregressor__regressor__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "sr_model_gs = GridSearchCV(\n",
    "    sr_model,\n",
    "    sr_model_params,\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "sr_model_gs.fit(X_tr_p0, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "sr_y_pred = sr_model_gs.predict(X_te_p0)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, sr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, sr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params:\\n\",sr_model_gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend Ridge, Lasso, or ElasticNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: testing polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p1 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p1 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p.shape)\n",
    "print(\"X_te:\", X_te_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "lr_model.fit(X_tr_p1, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p1)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p2 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"], 2)\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p2 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"], 2)\n",
    "\n",
    "print(\"X_tr:\", X_tr_p2.shape)\n",
    "print(\"X_te:\", X_te_p2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessedtrain dataset\n",
    "lr_model.fit(X_tr_p2, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p2)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p3 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p3 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "print(\"X_tr:\", X_tr_p3.shape)\n",
    "print(\"X_te:\", X_te_p3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "lr_model.fit(X_tr_p3, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p3)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: testing different feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p4 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p4 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "print(\"X_tr:\", X_tr_p4.shape)\n",
    "print(\"X_te:\", X_te_p4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday data\n",
    "X_tr_p4_sub1 = X_tr_p4.drop(columns=[col for col in X_tr_p4.columns if \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday data\n",
    "X_te_p4_sub1 = X_te_p4.drop(columns=[col for col in X_te_p4.columns if \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p4_sub1.shape)\n",
    "print(\"X_te:\", X_te_p4_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the train dataset\n",
    "lr_model.fit(X_tr_p4_sub1, y_tr)\n",
    "\n",
    "# Predict the target variable of the test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p4_sub1)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without weather forecast data\n",
    "X_tr_p4_sub2 = X_tr_p4.drop(columns=[col for col in X_tr_p4.columns if \"wf\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without weather forecast data\n",
    "X_te_p4_sub2 = X_te_p4.drop(columns=[col for col in X_te_p4.columns if \"wf\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p4_sub2.shape)\n",
    "print(\"X_te:\", X_te_p4_sub2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the train dataset\n",
    "lr_model.fit(X_tr_p4_sub2, y_tr)\n",
    "\n",
    "# Predict the target variable of the test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p4_sub2)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday and weather forecastdata\n",
    "X_tr_p4_sub3 = X_tr_p4.drop(columns=[col for col in X_tr_p4.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday and weather forecast data\n",
    "X_te_p4_sub3 = X_te_p4.drop(columns=[col for col in X_te_p4.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p4_sub3.shape)\n",
    "print(\"X_te:\", X_te_p4_sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to linear regression model\n",
    "lr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LinearRegression(), func=np.log, inverse_func=np.exp\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the train dataset\n",
    "lr_model.fit(X_tr_p4_sub3, y_tr)\n",
    "\n",
    "# Predict the target variable of the test dataset\n",
    "lr_y_pred = lr_model.predict(X_te_p4_sub3)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, lr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, lr_y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Linear Regression: Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the train dataset\n",
    "X_tr_p5 = preprocess(X_tr, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "# Apply preprocessing to the train dataset\n",
    "X_te_p5 = preprocess(X_te, categorical_cols, continuous_cols, [\"trip_distance\"], 3)\n",
    "\n",
    "print(\"X_tr:\", X_tr_p5.shape)\n",
    "print(\"X_te:\", X_te_p5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday and weather forecastdata\n",
    "X_tr_p5_sub3 = X_tr_p5.drop(columns=[col for col in X_tr_p4.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday and weather forecast data\n",
    "X_te_p5_sub3 = X_te_p5.drop(columns=[col for col in X_te_p4.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_p5_sub3.shape)\n",
    "print(\"X_te:\", X_te_p5_sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to Ridge regression model\n",
    "sr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=SGDRegressor(alpha=0.001, penalty=\"l1\"),\n",
    "        func=np.log,\n",
    "        inverse_func=np.exp,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "sr_model.fit(X_tr_p5_sub3, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters\n",
    "sr_y_pred = sr_model.predict(X_te_p5_sub3)\n",
    "\n",
    "print(\"The MSE of the linear regression model is: {:.0f}\".format(MSE(y_te, sr_y_pred)))\n",
    "print(\"The MAE of the linear regression model is: {:.0f}\".format(MAE(y_te, sr_y_pred)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBFy6LDJ28+i+mv+3BWFx7",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01.data",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
