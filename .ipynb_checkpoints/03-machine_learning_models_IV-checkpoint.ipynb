{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcp6PULBCqss"
   },
   "source": [
    "# 5. Capstone Project: Machine Learning Models IV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxLOusgfEwks"
   },
   "source": [
    "***\n",
    "\n",
    "![headerall](./images/headers/header_all.jpg)\n",
    "\n",
    "##  Goals\n",
    "\n",
    "### Project:\n",
    "In this work, we will first analyze where and when traffic congestion is highest and lowest in New York State. We will then build different machine learning models capable of predicting cab travel times in and around New York City using only variables that can be easily obtained from a smartphone app or a website. We will then compare their performance and explore the possibility of using additional variables such as weather forecasts and holidays to improve the predictive performance of the models.\n",
    "\n",
    "### Section:\n",
    "In this section, we will use the knowledge gained during the exploratory data analysis to perform the final feature transformation. Next, we will create and compare the performance of several machine learning models, namely: linear regressions, a support vector machine regressor, a random forest regressor and a gradient boosted decision tree. The feature space and hyperparameters will be optimised for each model to obtain the best possible performance.\n",
    "\n",
    "## Data\n",
    "### External Datasets:\n",
    "- Weather Forecast: The 2018 NYC weather forecast was collected from the [National Weather Service Forecast Office](https://w2.weather.gov/climate/index.php?wfo=okx) website. Daily measurements were taken from January to December 2018 in Central Park. These measures are given in imperial units and include daily minimum and maximum temperatures, precipitations, snowfall, and snow depth.\n",
    "\n",
    "- Holidays: The 2018 NYC holidays list was collected from the [Office Holiday](https://www.officeholidays.com/countries/usa/new-york/2021) website. The dataset contains the name, date, and type of holidays for New York.\n",
    "\n",
    "- Taxi Zones: The NYC Taxi Zones dataset was collected from the [NYC Open Data](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc) website. It contains the pickup and drop-off zones (Location IDs) for the Yellow, Green, and FHV Trip Records. The taxi zones are based on the NYC Department of City Planning’s Neighborhood.\n",
    "\n",
    "### Primary Datasets:\n",
    "\n",
    "- Taxi Trips: The 2018 NYC Taxi Trip dataset was collected from the [Google Big Query](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?project=jovial-monument-300209&folder=&organizationId=) platform. The dataset contains more than 100'000'000 Yellow Taxi Trip records for 2018 and contains an extensive amount of variables including the pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8tvC_V2FEj5"
   },
   "source": [
    "***\n",
    "## Table of Content:\n",
    "    1. Data Preparation\n",
    "        1.1 External Datasets\n",
    "            1.1.1 Weather Forecast Dataset\n",
    "            1.1.2 Holidays Dataset\n",
    "            1.1.3 Taxi Zones Dataset\n",
    "        1.2 Primary Dataset\n",
    "            1.2.1 Taxi Trips Dataset\n",
    "            1.2.2 Taxi Trips Subset\n",
    "    2. Exploratory Data Analysis\n",
    "        2.1 Primary Dataset\n",
    "            2.1.1 Temporal Analysis\n",
    "            2.1.2 Spatio-Temporal Analysis\n",
    "        2.2 External Datasets\n",
    "            2.2.1 Temporal Analysis of Weather Data\n",
    "            2.2.2 Temporal Analysis of Holidays Data\n",
    "        2.3 Combined Dataset\n",
    "            2.3.1 Overall Features Correlation\n",
    "    3. Machine Learning Models\n",
    "        3.1 Data Preparation\n",
    "        3.2 Baselines\n",
    "        3.3 Model Training\n",
    "            3.3.1 Linear Regression\n",
    "            3.3.2 Support Vector Machine\n",
    "            3.3.3 Random Forest Regressor\n",
    "            3.3.4 Gradient Boosted Decision Tree\n",
    "            3.3.5 Final Comparison\n",
    "    5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1WFf7qqEi03",
    "tags": []
   },
   "source": [
    "***\n",
    "## Python Libraries and Magic commands Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import python core libraries\n",
    "import time\n",
    "\n",
    "# Import data processing libraries gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import median_absolute_error as MAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up magic commands\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: (824654, 33)\n",
      "y_tr: (824654,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the train dataset\n",
    "train_df = pd.read_pickle(r'data/processed/train.pickle')\n",
    "\n",
    "# Get the independant variables from the train dataset\n",
    "X_tr = train_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the train dataset\n",
    "y_tr = train_df[\"trip_duration\"]\n",
    "\n",
    "print('X_tr:', X_tr.shape)\n",
    "print('y_tr:', y_tr.shape, y_tr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_te: (206156, 33)\n",
      "y_te: (206156,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the test dataset\n",
    "test_df = pd.read_pickle(r'data/processed/test.pickle')\n",
    "\n",
    "# Get the independant variables from the test dataset\n",
    "X_te = test_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the test dataset\n",
    "y_te = test_df[\"trip_duration\"]\n",
    "\n",
    "print('X_te:', X_te.shape)\n",
    "print('y_te:', y_te.shape, y_te.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get id column names from the train dataset\n",
    "id_cols = [c for c in train_df.columns if \"_id\" in c]\n",
    "\n",
    "# Remove ID features in the train dataset\n",
    "X_tr.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Remove ID features in the test dataset\n",
    "X_te.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the train dataset\n",
    "X_tr.drop(\"pickup_yearday\", axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the test dataset\n",
    "X_te.drop(\"pickup_yearday\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Functions Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that performs preprocessing steps to the selected dataset\n",
    "def preprocess(data, categorical_cols, continuous_cols, transform_cols, polynome_deg=1):\n",
    "\n",
    "    # Create a copy of the data frame\n",
    "    df = data.copy()\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "    # Log-transform numerical variables\n",
    "    for col in transform_cols:\n",
    "        df[col] = np.log(df[col])\n",
    "    \n",
    "    # Add polynomial features\n",
    "    for col in continuous_cols:\n",
    "        if polynome_deg > 1:\n",
    "            for poly in range(polynome_deg + 1):\n",
    "                df[\"{}**{}\".format(col, poly)] = df[col] ** poly\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that display individual barplot\n",
    "def plot_barplot(\n",
    "    data,\n",
    "    x_var,\n",
    "    y_var,\n",
    "    xlabel,\n",
    "    ylabel,\n",
    "    title=\"\",\n",
    "    labels=None,\n",
    "    label_order=None,\n",
    "    legend=None,\n",
    "    figsize=(20, 5),\n",
    "    palette=\"YlGnBu\",\n",
    "):\n",
    "    # Create a figure with one column and row\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # Generate the plot\n",
    "    if labels == None:\n",
    "        sns.barplot(\n",
    "            x=x_var,\n",
    "            y=y_var,\n",
    "            data=data,\n",
    "            ax=ax,\n",
    "            palette=palette,\n",
    "        )\n",
    "    else:\n",
    "        sns.barplot(\n",
    "            x=x_var,\n",
    "            y=y_var,\n",
    "            hue=labels,\n",
    "            hue_order=label_order,\n",
    "            data=data,\n",
    "            ax=ax,\n",
    "            palette=palette,\n",
    "        )\n",
    "        ax.legend(title=legend)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"left\"].set_visible(False)\n",
    "    ax.yaxis.set_ticks_position(\"none\")\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.grid()\n",
    "\n",
    "    # Adjust the padding between and around subplots\n",
    "    fig.tight_layout(pad=0.5, w_pad=0.5)\n",
    "\n",
    "    # Add a title to the figure\n",
    "    plt.title(title.title(), fontsize=18, pad=15)\n",
    "    \n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## 3.3.3 Model Training: Random Forest Regressor\n",
    "\n",
    "## Goals:\n",
    "\n",
    "## Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Regressor: testing different training sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the Random Forest regression model is: 35\n",
      "The MAE of the Random Forest regression model is: 3\n",
      "\n",
      "(Exectution time: 1 sec)\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr[:100], y_tr[:100])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_1 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the Random Forest regression model is: 28\n",
      "The MAE of the Random Forest regression model is: 2\n",
      "\n",
      "(Exectution time: 2 sec)\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr[:1000], y_tr[:1000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_2 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the Random Forest regression model is: 22\n",
      "The MAE of the Random Forest regression model is: 2\n",
      "\n",
      "(Exectution time: 8 sec)\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr[:10000], y_tr[:10000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_3 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the Random Forest regression model is: 18\n",
      "The MAE of the Random Forest regression model is: 2\n",
      "\n",
      "(Exectution time: 62 sec)\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr[:100000], y_tr[:100000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_4 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** subsample dataset and decrease/optimise feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Random Forest Regressor: testing different feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the features importance\n",
    "features_imp_df = pd.DataFrame({'features':X_tr.columns, 'importance':rf_model['randomforestregressor'].feature_importances_})\n",
    "\n",
    "# Sort feature by importance in the data frame\n",
    "features_imp_df.sort_values('importance', ascending=False, inplace=True)\n",
    "\n",
    "# Display the feature importance in an horizontal barplot\n",
    "plot_barplot(\n",
    "   features_imp_df,\n",
    "    'importance',\n",
    "    'features',\n",
    "    xlabel='Importance',\n",
    "    ylabel='Features',\n",
    "    figsize=(15, 7.5),\n",
    "    palette=\"Blues_r\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**  Nonlinear SVM is a black box classifier for which we do not know the mapping function Φ explicitly. Thus, the weight vector w cannot be explicitly computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday data\n",
    "X_tr_sub1 = X_tr.drop(columns=[col for col in X_tr.columns if \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday data\n",
    "X_te_sub1 = X_te.drop(columns=[col for col in X_te.columns if \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub1.shape)\n",
    "print(\"X_te:\", X_te_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model_sub1 = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model_sub1.fit(X_tr_sub1, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model_sub1.predict(X_te_sub1)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without weather forecast data\n",
    "X_tr_sub2 = X_tr.drop(columns=[col for col in X_tr.columns if \"wf\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without weather forecast data\n",
    "X_te_sub2 = X_te.drop(columns=[col for col in X_te.columns if \"wf\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub2.shape)\n",
    "print(\"X_te:\", X_te_sub2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model_sub2 = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the prerocessed train dataset\n",
    "rf_model_sub2.fit(X_tr_sub2, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model_sub2.predict(X_te_sub2)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday and weather forecastdata\n",
    "X_tr_sub3 = X_tr.drop(columns=[col for col in X_tr.columns if \"wf\" in col or \"holiday\" in col])\n",
    "                               \n",
    "# Create a subset of the test matrix without holiday and weather forecast data\n",
    "X_te_sub3 = X_te.drop(columns=[col for col in X_te.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub3.shape)\n",
    "print(\"X_te:\", X_te_sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model_sub3 = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model_sub3.fit(X_tr_sub3, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model_sub3.predict(X_te_sub3)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Improve the accuracy with which the model is able to predict for new data.\n",
    "Reduce computational cost.\n",
    "Produce a more interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Regressor : testing dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=0.95),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_1 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=0.99),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model.fit(X_tr, y_tr)\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_1 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can in fact: speed up training, avoid overfitting and ultimately lead to better classification results thanks to the reduced noise in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Random Forest Regressor: testing different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "rf_model_params = {\n",
    "    'max_depth': [None, 20, 40, 60,  80, 100],\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "rf_model_gs = GridSearchCV(\n",
    "    rf_model,\n",
    "    rf_model_params,\n",
    "    cv=3,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model_gs.fit(X_tr[:80], y_tr[:80])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model_gs.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_1 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print('\\n(Exectution time: {:.0f} sec)'.format(end_time_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to a Random Forest model\n",
    "rf_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    RandomForestRegressor(random_state=0)\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "rf_model_params = {\n",
    "    'max_depth': [None, 20, 40, 60,  80, 100],\n",
    "    'n_estimators': np.logspace(1, 3, 10),\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "rf_model_gs = GridSearchCV(\n",
    "    rf_model,\n",
    "    rf_model_params,\n",
    "    cv=3,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "rf_model_gs.fit(X_tr[:80], y_tr[:80])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "rf_y_pred = rf_model_gs.predict(X_te)\n",
    "\n",
    "print('The MSE of the Random Forest regression model is: {:.0f}'.format(MSE(y_te, rf_y_pred)))\n",
    "print('The MAE of the Random Forest regression model is: {:.0f}'.format(MAE(y_te, rf_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params: \\n\", rf_y_pred.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is usually suggested to use linear kernels if the number of features is larger than the number of observations in the dataset (otherwise RBF might be a better choice). When working with a large amount of data using RBF, speed might become a constraint to take into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBFy6LDJ28+i+mv+3BWFx7",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01.data",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
