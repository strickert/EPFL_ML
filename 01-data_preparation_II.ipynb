{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcp6PULBCqss"
   },
   "source": [
    "# 5. Capstone Project: Data Preparation II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxLOusgfEwks",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "![headerall](./images/headers/header_all.jpg)\n",
    "\n",
    "##  Goals\n",
    "\n",
    "### Project:\n",
    "In this work, we will first analyze where and when traffic congestion is highest and lowest in New York State. We will then build different machine learning models capable of predicting cab travel times in and around New York City using only variables that can be easily obtained from a smartphone app or a website. We will then compare their performance and explore the possibility of using additional variables such as weather forecasts and holidays to improve the predictive performance of the models.\n",
    "\n",
    "### Section:\n",
    "In this section, data will be prepared for the next steps, i.e., exploratory data analysis and machine learning model building. First, we will collect and import the necessary datasets. Next, we will review each of these datasets to fully understand the data and its structure, including the different variables available, their format, and their relevance to the project. Finally, we will perform data cleaning and manipulation, as well as feature engineering, and encoding.\n",
    "\n",
    "The data preparation will be divided into two subsections, the first dedicated to the external datasets (Data Preparation Part I) and the second to the New York City Taxi Trip data set (Data Preparation Part II). Finally, external datasets will be merged with the primary one.\n",
    "\n",
    "## Data\n",
    "### External Datasets:\n",
    "- Weather Forecast: The 2018 NYC weather forecast was collected from the [National Weather Service Forecast Office](https://w2.weather.gov/climate/index.php?wfo=okx) website. Daily measurements were taken from January to December 2018 in Central Park. These measures are given in imperial units and include daily minimum and maximum temperatures, precipitations, snowfall, and snow depth.\n",
    "\n",
    "- Holidays: The 2018 NYC holidays list was collected from the [Office Holiday](https://www.officeholidays.com/countries/usa/new-york/2021) website. The dataset contains the name, date, and type of holidays for New York.\n",
    "\n",
    "- Taxi Zones: The NYC Taxi Zones dataset was collected from the [NYC Open Data](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc) website. It contains the pickup and drop-off zones (Location IDs) for the Yellow, Green, and FHV Trip Records. The taxi zones are based on the NYC Department of City Planningâ€™s Neighborhood.\n",
    "\n",
    "### Primary Datasets:\n",
    "\n",
    "- Taxi Trips: The 2018 NYC Taxi Trip dataset was collected from the [Google Big Query](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?project=jovial-monument-300209&folder=&organizationId=) platform. The dataset contains more than 100'000'000 Yellow Taxi Trip records for 2018 and contains an extensive amount of variables including the pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8tvC_V2FEj5"
   },
   "source": [
    "***\n",
    "## Table of Content:\n",
    "    1. Data Preparation\n",
    "        1.1 External Datasets\n",
    "            1.1.1 Weather Forecast Dataset\n",
    "            1.1.2 Holidays Dataset\n",
    "            1.1.3 Taxi Zones Dataset\n",
    "        1.2 Primary Dataset\n",
    "            1.2.1 Taxi Trips Dataset\n",
    "    2. Exploratory Data Analysis\n",
    "        2.1 Primary Dataset\n",
    "            2.1.1 Temporal Analysis\n",
    "            2.1.2 Spatio-Temporal Analysis\n",
    "        2.2 External Datasets\n",
    "            2.2.1 Temporal Analysis of Weather Data\n",
    "            2.2.2 Temporal Analysis of Holidays Data\n",
    "        2.3 Combined Dataset\n",
    "            2.3.1 Overall Features Correlation\n",
    "    3. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Python Libraries and Magic commands Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python core libraries\n",
    "import os\n",
    "\n",
    "# Import data processing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization librairies\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up magic commands\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1WFf7qqEi03",
    "tags": []
   },
   "source": [
    "***\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the weather forecasts dataset\n",
    "weather_df = pd.read_pickle(r'data/processed/weather.pickle')\n",
    "\n",
    "# Import the holidays dataset\n",
    "holidays_df = pd.read_pickle(r'data/processed/holidays.pickle')\n",
    "\n",
    "# Import the zones dataset\n",
    "zones_df = pd.read_pickle(r'data/processed/zones.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Functions import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that transform column headers to titles\n",
    "def to_title(data, sep='_'):\n",
    "    return [s.replace(sep, ' ').title() for s in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that compute and display descriptive statistics for selected columns\n",
    "def outliers_distribution(data, columns, statistics, percentiles):\n",
    "\n",
    "    # Create an empty dictionnary to collect the columns' descriptive statistics\n",
    "    frame = {}\n",
    "\n",
    "    # Iterate over the given numerical columns\n",
    "    for column in columns:\n",
    "\n",
    "        # Compute the descriptive statistics for records above the column's given percentile\n",
    "        output = (\n",
    "            data.loc[data[column] > percentiles[column].iloc[0], column]\n",
    "            .describe()[statistics]\n",
    "            .round(2)\n",
    "        )\n",
    "\n",
    "        # Add the column's descriptive statistics to the dictionnary\n",
    "        frame[column] = output\n",
    "\n",
    "    # Return the columns' descriptive statistics as a data frame\n",
    "    return pd.DataFrame(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that display scatter plots of selected numerical columns\n",
    "def plot_scatter(data, columns, colnames, variable, ncols=1, nrows=1):\n",
    "\n",
    "    varname = variable.replace(\"_\", \" \").title()\n",
    "\n",
    "    # Create a figure with n columns and rows\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, sharey=True, figsize=(ncols * 5, nrows * 5)\n",
    "    )\n",
    "\n",
    "    # Flatten axes for 2D figures\n",
    "    if ncols > 1 or nrows > 1:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    # Generate the plots\n",
    "    for ax, col, colname in zip(axes, columns, colnames):\n",
    "        sns.scatterplot(x=col, y=variable, data=data, ax=ax, s=10)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.set_ylim(0)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_xlabel(colname, fontsize=14)\n",
    "        ax.set_ylabel(varname, fontsize=14)\n",
    "        ax.set_title(f\"{varname} vs {colname}\", fontsize=16)\n",
    "        ax.grid()\n",
    "\n",
    "    # Adjust the padding between and around subplots\n",
    "    fig.tight_layout(pad=0.5, w_pad=0.5)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that display scatter plots of selected numerical columns\n",
    "def plot_histplot(data, columns, colnames, ncols=1, nrows=1):\n",
    "\n",
    "    # Create a figure with n columns and rows\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=nrows, ncols=ncols, sharey=True, figsize=(ncols * 5, nrows * 5)\n",
    "    )\n",
    "\n",
    "    # Flatten axes for 2D figures\n",
    "    if ncols > 1 or nrows > 1:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    # Generate the plots\n",
    "    for ax, col, colname in zip(axes, columns, colnames):\n",
    "        sns.histplot(x=col, bins=20, data=data, ax=ax, edgecolor=\"white\", alpha=0.7)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_xlabel(colname, fontsize=14)\n",
    "        ax.set_ylabel(\"Count\", fontsize=14)\n",
    "        ax.set_title(colname, fontsize=16)\n",
    "        ax.grid()\n",
    "\n",
    "    # Adjust the padding between and around subplots\n",
    "    fig.tight_layout(pad=0.5, w_pad=0.5)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that display individual boxplot of selected numerical columns\n",
    "def plot_boxplot(data, columns, colnames, ncols=1, nrows=1):\n",
    "    # Create a figure with n columns and rows\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "\n",
    "    # Flatten axes for 2D figures\n",
    "    if ncols > 1 or nrows > 1:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    # Generate the plots\n",
    "    for ax, col, colname in zip(axes, columns, colnames):\n",
    "        sns.boxplot(\n",
    "            y=col, data=data, ax=ax, width=0.35, linewidth=1, boxprops=dict(alpha=0.7)\n",
    "        )\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position(\"none\")\n",
    "        ax.xaxis.set_ticks([])\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_xlabel(colname, fontsize=14)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.set_title(colname, fontsize=16)\n",
    "        ax.grid()\n",
    "\n",
    "    # Adjust the padding between and around subplots\n",
    "    fig.tight_layout(pad=0.5, w_pad=0.5)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that display individual violinplot of selected numerical columns\n",
    "def plot_violinplot(data, columns, colnames, ncols=1, nrows=1, color=\"C0\"):\n",
    "\n",
    "    # Create a figure with n columns and rows\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 5, nrows * 5))\n",
    "\n",
    "    # Flatten axes for 2D figures\n",
    "    if ncols > 1 or nrows > 1:\n",
    "        axes = axes.ravel()\n",
    "\n",
    "    # Generate the plots\n",
    "    for ax, col, colname in zip(axes, columns, colnames):\n",
    "        violin_plot = ax.violinplot(data[col])\n",
    "\n",
    "        # Change color of the violin plot\n",
    "        for pc in violin_plot[\"bodies\"]:\n",
    "            pc.set_facecolor(color)\n",
    "            pc.set_edgecolor(color)\n",
    "\n",
    "        violin_plot[\"cbars\"].set_edgecolor(\"black\")\n",
    "        violin_plot[\"cmins\"].set_edgecolor(\"black\")\n",
    "        violin_plot[\"cmaxes\"].set_edgecolor(\"black\")\n",
    "\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.yaxis.set_ticks_position(\"none\")\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.set_title(colname, fontsize=16)\n",
    "        ax.grid()\n",
    "\n",
    "    # Adjust the padding between and around subplots\n",
    "    fig.tight_layout(pad=0.5, w_pad=0.5)\n",
    "\n",
    "    # Display the figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# 1. Data Preparation\n",
    "## 1.2 Data Preparation: NYC taxi trip dataset\n",
    "### Goal:\n",
    "Explore a subset (10%) of the 2018 NYC yellow taxi trip dataset dataset and perform data cleaning and manipulation, as well as feature engineering and encoding if needed.\n",
    "### Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 0 to 10000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [25:50<00:00, 6451.24rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 10000000 to 20000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [24:53<00:00, 6696.77rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 20000000 to 30000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [22:47<00:00, 7310.80rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 30000000 to 40000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [25:47<00:00, 6462.82rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 40000000 to 50000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [24:34<00:00, 6781.51rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 50000000 to 60000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [23:09<00:00, 7194.58rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 60000000 to 70000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [24:09<00:00, 6900.74rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 70000000 to 80000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000000/10000000 [23:38<00:00, 7051.84rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching rows from 80000000 to 90000000:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž         | 3517983/10000000 [38:09<14:21, 7526.53rows/s]"
     ]
    },
    {
     "ename": "GenericGBQException",
     "evalue": "Reason: Deadline of 600.0s exceeded while calling functools.partial(functools.partial(<bound method JSONConnection.api_request of <google.cloud.bigquery._http.Connection object at 0x10cd61880>>, timeout=300.0, method='GET', path='/projects/exts-ml/datasets/_888f04df2bf18df6709faaa7f2ca1793859c041d/tables/anonf5e544d497bfcceded2fcf1e334537a57e13d71c/data', query_params={'pageToken': 'BEOATIW6PMAQAAASAUIIBAEAAUNAWCE73TLACEH777776BZA77777777777767ZKBEEAAEADDDTJJNYB', 'formatOptions.useInt64Timestamp': True}, headers={'X-Server-Timeout': '300.0', 'Accept-Encoding': 'gzip', 'X-Goog-API-Client': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0', 'User-Agent': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0'})), last exception: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_chunk_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    750\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_chunked_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0;31m# there is yet no clean way to get at it from this context.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Read timed out.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadTimeoutError\u001b[0m: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36mapi_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         response = self._make_request(\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, method, url, data, content_type, headers, target_object, timeout)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         return self._do_request(\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_object\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/_http.py\u001b[0m in \u001b[0;36m_do_request\u001b[0;34m(self, method, url, headers, data, target_object, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \"\"\"\n\u001b[0;32m--> 375\u001b[0;31m         return self.http.request(\n\u001b[0m\u001b[1;32m    376\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mcontent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mReadTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36m_download_results\u001b[0;34m(self, query_job, max_results, progress_bar_type, user_dtypes)\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mconversion_dtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_dtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             df = rows_iter.to_dataframe(\n\u001b[0m\u001b[1;32m    534\u001b[0m                 \u001b[0mdtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversion_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object, geography_as_object)\u001b[0m\n\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1996\u001b[0;31m         record_batch = self.to_arrow(\n\u001b[0m\u001b[1;32m   1997\u001b[0m             \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36mto_arrow\u001b[0;34m(self, progress_bar_type, bqstorage_client, create_bqstorage_client)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mrecord_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m             for record_batch in self._to_arrow_iterable(\n\u001b[0m\u001b[1;32m   1795\u001b[0m                 \u001b[0mbqstorage_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbqstorage_client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36m_to_page_iterable\u001b[0;34m(self, bqstorage_download, tabledata_list_download, bqstorage_client)\u001b[0m\n\u001b[1;32m   1691\u001b[0m         )\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mresult_pages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/_pandas_helpers.py\u001b[0m in \u001b[0;36mdownload_arrow_row_iterator\u001b[0;34m(pages, bq_schema)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0m_row_iterator_page_to_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marrow_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_page_iter\u001b[0;34m(self, increment)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/api_core/page_iterator.py\u001b[0m in \u001b[0;36m_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_next_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_page_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/table.py\u001b[0m in \u001b[0;36m_get_next_page_response\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1646\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"maxResults\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_page_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1647\u001b[0;31m         return self.api_request(\n\u001b[0m\u001b[1;32m   1648\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HTTP_METHOD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/cloud/bigquery/client.py\u001b[0m in \u001b[0;36m_call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             )\n\u001b[0;32m--> 281\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, deadline, on_error)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeadline_datetime\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 six.raise_from(\n\u001b[0m\u001b[1;32m    200\u001b[0m                     exceptions.RetryError(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mRetryError\u001b[0m: Deadline of 600.0s exceeded while calling functools.partial(functools.partial(<bound method JSONConnection.api_request of <google.cloud.bigquery._http.Connection object at 0x10cd61880>>, timeout=300.0, method='GET', path='/projects/exts-ml/datasets/_888f04df2bf18df6709faaa7f2ca1793859c041d/tables/anonf5e544d497bfcceded2fcf1e334537a57e13d71c/data', query_params={'pageToken': 'BEOATIW6PMAQAAASAUIIBAEAAUNAWCE73TLACEH777776BZA77777777777767ZKBEEAAEADDDTJJNYB', 'formatOptions.useInt64Timestamp': True}, headers={'X-Server-Timeout': '300.0', 'Accept-Encoding': 'gzip', 'X-Goog-API-Client': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0', 'User-Agent': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0'})), last exception: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/rwdq78h925zdjg_0ccng3f4r0000gn/T/ipykernel_3489/1499231963.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m      \"\"\"\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Get a chunck of the Google Big Query servers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     df = pd.read_gbq(\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"exts-ml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdialect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"standard\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tqdm\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas/io/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, progress_bar_type)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# END: new kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     return pandas_gbq.read_gbq(\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, verbose, private_key, progress_bar_type, dtypes)\u001b[0m\n\u001b[1;32m    850\u001b[0m     )\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m     final_df = connector.run_query(\n\u001b[0m\u001b[1;32m    853\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mconfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mdtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dtypes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         return self._download_results(\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0mquery_reply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mmax_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36m_download_results\u001b[0;34m(self, query_job, max_results, progress_bar_type, user_dtypes)\u001b[0m\n\u001b[1;32m    537\u001b[0m             )\n\u001b[1;32m    538\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/geo_env/lib/python3.9/site-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;31m# <https://cloud.google.com/bigquery/troubleshooting-errors>`__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     def run_query(\n",
      "\u001b[0;31mGenericGBQException\u001b[0m: Reason: Deadline of 600.0s exceeded while calling functools.partial(functools.partial(<bound method JSONConnection.api_request of <google.cloud.bigquery._http.Connection object at 0x10cd61880>>, timeout=300.0, method='GET', path='/projects/exts-ml/datasets/_888f04df2bf18df6709faaa7f2ca1793859c041d/tables/anonf5e544d497bfcceded2fcf1e334537a57e13d71c/data', query_params={'pageToken': 'BEOATIW6PMAQAAASAUIIBAEAAUNAWCE73TLACEH777776BZA77777777777767ZKBEEAAEADDDTJJNYB', 'formatOptions.useInt64Timestamp': True}, headers={'X-Server-Timeout': '300.0', 'Accept-Encoding': 'gzip', 'X-Goog-API-Client': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0', 'User-Agent': 'pandas-1.3.1 gl-python/3.9.6 grpc/1.38.1 gax/1.22.4 gapic/2.26.0 gccl/2.26.0'})), last exception: HTTPSConnectionPool(host='bigquery.googleapis.com', port=443): Read timed out."
     ]
    }
   ],
   "source": [
    "# Create a global variable containing the credentials to query Google Big Query Servers\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = r\"credentials\\exts-ml-5e5656f1d391.json\"\n",
    "\n",
    "# Create an empty data frame\n",
    "records_df = pd.DataFrame()\n",
    "\n",
    "# Create a row offest\n",
    "row_offset = 0\n",
    "\n",
    "# Create a row limit\n",
    "row_limit = 10000000\n",
    "\n",
    "# Create a row offest counter\n",
    "row_counter = 10000000\n",
    "\n",
    "# Get the NYC taxi dataset in chuncks of 10M to avoid insufficient memory issues\n",
    "while row_counter == row_limit:\n",
    "\n",
    "    print(f\"Fetching rows from {row_offset} to {row_offset + row_counter}:\")\n",
    "\n",
    "    # Create a query to fetch and transform the NYC taxi dataset from the Google Big Query servers\n",
    "    query = f\"\"\"\n",
    "     SELECT\n",
    "         pickup_datetime,\n",
    "         dropoff_datetime,\n",
    "         passenger_count,\n",
    "         trip_distance,\n",
    "         tolls_amount,\n",
    "         fare_amount,\n",
    "         pickup_location_id,\n",
    "         dropoff_location_id,\n",
    "         TIMESTAMP_DIFF(TIMESTAMP(pickup_datetime), TIMESTAMP(dropoff_datetime), MINUTE) as trip_duration\n",
    "     FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2018`\n",
    "     LIMIT {row_limit}\n",
    "     OFFSET {row_offset};\n",
    "     \"\"\"\n",
    "    # Get a chunck of the Google Big Query servers\n",
    "    df = pd.read_gbq(\n",
    "        query, project_id=\"exts-ml\", dialect=\"standard\", progress_bar_type=\"tqdm\"\n",
    "    )\n",
    "\n",
    "    # Get a random sample of 1% of the NYC taxi dataset chunck\n",
    "    records_df = records_df.append(\n",
    "        df.sample(frac=0.010, replace=False, random_state=0), ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Update the row counter with the current data frame lenght\n",
    "    row_counter = len(df.index)\n",
    "\n",
    "    # Update the row offest\n",
    "    row_offset += row_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data frame\n",
    "#records_df = pd.read_pickle('data/raw/taxi_records_v1.pickle')\n",
    "\n",
    "#records_df['trip_duration'] = (\n",
    "#   records_df.dropoff_datetime - records_df.pickup_datetime\n",
    "#).dt.seconds / 60\n",
    "\n",
    "#records_df['pickup_location_id'] = records_df['pickup_location_id'].astype('int64')\n",
    "#records_df['dropoff_location_id'] = records_df['dropoff_location_id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first five rows of the data frame\n",
    "records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the data frame contains 1% of all 2018 NYC cab fare records, totaling 11,223,463 data points. Most of the numeric columns are the wrong type and will need to be converted from string (Object) to float or integer. Columns may also contain null values, and since most machine learning algorithms cannot properly process them, we will first need to check that each column does not contain any before continuing with data cleaning and feature engineering/coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the number of empty cells in each column of the data frame\n",
    "records_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the dataset does not contain any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns to be converted to float\n",
    "num_cols = ['trip_distance', 'tolls_amount', 'fare_amount', 'trip_duration']\n",
    "\n",
    "# Convert numerical columns to float\n",
    "records_df[num_cols] = records_df[num_cols].astype('float64')\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the columns have been correctly converted to float. Values of numeric columns will be converted to metric units. In addition, columns containing dates will be checked for records dating from before or after 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the trip distance from miles to kilometers\n",
    "records_df[\"trip_distance\"] = np.multiply(records_df[\"trip_distance\"], 1.60934)\n",
    "\n",
    "# Display the records with pickup dates out of the date range (2018.01.01 - 2018.01.01)\n",
    "records_df[\n",
    "    (records_df['pickup_datetime'] < '2018.01.01')\n",
    "    | (records_df['pickup_datetime'] > '2019.01.01')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** over a dozen records date from before or after 2018. These records will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records with pickup dates below 2018.01.01 and above 2018.01.01\n",
    "records_df = records_df[\n",
    "    (records_df['pickup_datetime'] >= '2018.01.01')\n",
    "    & (records_df['pickup_datetime'] <= '2019.01.01')\n",
    "]\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the 16 records with dates out of range were removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some descriptive statistics\n",
    "records_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Most numerical columns contain extreme and erroneous values. For instance, according to the [official NYC Taxi](https://www1.nyc.gov/site/tlc/passengers/passenger-frequently-asked-questions.page) website, the maximum number of passengers allowed by law in a yellow taxi is four in a four-passenger taxi or five in a five-passenger taxi. Therefore, trips with more than five passengers are either errors or the result of taxi drivers picking up additional passengers en route to their final destination. Some records have negative, null or extreme values the number of passenger, distance traveled, toll amount and fare amount. Taxi trip records with null, extreme or erroneous values will be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the name of numerical columns\n",
    "num_cols = [\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tolls_amount',\n",
    "    'fare_amount',\n",
    "    'trip_duration',\n",
    "]\n",
    "\n",
    "# Define the name of numerical columns\n",
    "num_colnames = to_title(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scatterplot of numerical columns\n",
    "plot_scatter(records_df, num_cols, num_colnames, 'trip_duration', ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histograms of numerical columns\n",
    "plot_histplot(records_df, num_cols, num_colnames, ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display boxplots of numerical columns\n",
    "plot_boxplot(records_df, num_cols, num_colnames, ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** The distribution of variables such as number of passengers, distance traveled, toll amount, trip amount, and trip duration is heavily skewed by the presence of many near-zero values and outliers. We will not eliminate the near-zero values because they correspond to real and frequent trips, but we will eliminate most of the outliers while trying to preserve some degree of variability so that our models can correctly estimate the duration of non-conventional taxi trips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the numerical columns' 75, 95, and 99th percentiles\n",
    "percentiles_df = records_df.quantile([0.75, 0.95, 0.99])[num_cols].reset_index()\n",
    "\n",
    "# Display the numerical columns' 75, 95, and 99th percentiles\n",
    "percentiles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the table above shows the 75th, 95th, and 99th of numerical variables, including passagenger count, trip distance, tolls and fare amounts, and trip duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the descriptive statistics to display\n",
    "perc_statistics = ['count', 'mean', 'std', 'min', 'max']\n",
    "\n",
    "# Get the numerical columns' 75th percentiles\n",
    "perc_75 = percentiles_df[percentiles_df['index'] == 0.75]\n",
    "\n",
    "# Compute and display descriptive statistics for records above the column's 75th percentile\n",
    "outliers_distribution(records_df, num_cols, perc_statistics, perc_75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the table above shows the descriptive statistics of the numerical variables for records marked as outliers, if the 75th percentile was used as the upper threshold. Depending on the variable, up to one-third of the data set could be didscarded and only the most common taxi trips would be retained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical columns' 95th percentiles\n",
    "perc_95 = percentiles_df[percentiles_df['index'] == 0.95]\n",
    "\n",
    "# Compute and display descriptive statistics for records above the column's 95th percentile\n",
    "outliers_distribution(records_df, num_cols, perc_statistics, perc_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the table above shows the descriptive statistics of the numerical variables for records marked as outliers, if the 95th percentile was used as the upper threshold. Depending on the variable, up to 7% of the data set could be discarded, but trips diversity would be greater than if the 75th percentile were used. It would also discard taxi trips with more than five passengers. However, for the other variables, using the 95th percentile would lead to the exclusion of long-distance and flat-rate rides, including those between Manhattan and JFK Airport, as reported in the [official NYC FAQ](https://www1.nyc.gov/site/tlc/passengers/taxi-fare.page)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the numerical columns' 99th percentiles\n",
    "perc_99 = percentiles_df[percentiles_df['index'] == 0.99]\n",
    "\n",
    "# Compute and display descriptive statistics for records above the column's 99th percentile\n",
    "outliers_distribution(records_df, num_cols, perc_statistics, perc_99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the table above shows the descriptive statistics of the numerical variables for records marked as outliers, if the 99th percentile was used as the upper threshold.  Depending on the variable, up to 1.5% of the data set could be discarded. The diversity of trips would be much greater than if the 75th and 95th percentiles were used. By using the 99th percentile, the most extreme or erroneous trips would be discarded, while retaining a high degree of variability, which would help our predictive model estimate the duration of non-conventional cab trips. We will thus use mostly the 99th and 95th percentil to delete outliers from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discards records with passenger counts above the 95th percentile and below or equal to 0\n",
    "records_df = records_df.loc[\n",
    "    (records_df['passenger_count'] <= percentiles_df['passenger_count'].iloc[1])\n",
    "    & (records_df['passenger_count'] > 0)\n",
    "]\n",
    "\n",
    "# Discards records with toll amounts above the 99th percentile and below or equal to 0\n",
    "records_df = records_df.loc[\n",
    "    (records_df['trip_distance'] <= percentiles_df['trip_distance'].iloc[2])\n",
    "    & (records_df['trip_distance'] > 0)\n",
    "]\n",
    "\n",
    "# Discards records with tolls amounts above the 99th percentile and below 0\n",
    "records_df = records_df.loc[\n",
    "    (records_df['tolls_amount'] <= percentiles_df['tolls_amount'].iloc[2])\n",
    "    & (records_df['trip_distance'] >= 0)\n",
    "]\n",
    "\n",
    "# Discards records with fare amounts above the 99th percentile and below 0\n",
    "records_df = records_df.loc[\n",
    "    (records_df['fare_amount'] <= percentiles_df['fare_amount'].iloc[2])\n",
    "    & (records_df['fare_amount'] > 0)\n",
    "]\n",
    "\n",
    "# Discards records with trip duration above the 99 percentile and below 1\n",
    "records_df = records_df.loc[\n",
    "    (records_df['trip_duration'] <= percentiles_df['trip_duration'].iloc[2])\n",
    "    & (records_df['trip_duration'] > 1)\n",
    "]\n",
    "\n",
    "# Display some descriptive statistics\n",
    "records_df.describe().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The 95th percentile was used to discard outliers from the passenger count column, while the 99th percentile was used to discard outliers and erroneous records from all other columns. Approximately 7% of the data set was discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scatterplots of numerical columns\n",
    "plot_scatter(records_df, num_cols, num_colnames, 'trip_duration', ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histograms of numerical columns\n",
    "plot_histplot(records_df, num_cols, num_colnames, ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display boxplots of numerical columns\n",
    "plot_boxplot(records_df, num_cols, num_colnames, ncols=len(num_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** After removing the extreme and spurious values from the data set, we can observe a strong correlation between travel time and distance and fare amount. In addition, each variable has a righ-skewed distribution. Therefore, we will need to normalize the data before using it to train our different models. Below, we will proceed with feature engineering and then encoding, starting with the date and time of pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column containing the date\n",
    "records_df[\"pickup_date\"] = records_df[\"pickup_datetime\"].dt.date\n",
    "\n",
    "# Add a new column containing the month of the year\n",
    "records_df[\"pickup_month\"] = records_df[\"pickup_datetime\"].dt.month\n",
    "\n",
    "# Add a new column containing the week of the year\n",
    "records_df[\"pickup_week\"] = records_df[\"pickup_datetime\"].dt.isocalendar().week\n",
    "\n",
    "# Add a new column containing the day of the year\n",
    "records_df[\"pickup_yearday\"] = records_df[\"pickup_datetime\"].dt.dayofyear\n",
    "\n",
    "# Add a new column containing the weekday of the week\n",
    "records_df[\"pickup_weekday\"] = records_df[\"pickup_datetime\"].dt.weekday\n",
    "\n",
    "# Add a new column containing the type of weekday (week=0, weekend=1)\n",
    "records_df[\"pickup_weekday_type\"] = np.where(records_df[\"pickup_weekday\"] < 5, 0, 1)\n",
    "\n",
    "# Add a new column containing the hour of the day\n",
    "records_df[\"pickup_hour\"] = records_df[\"pickup_datetime\"].dt.hour\n",
    "\n",
    "# Add a new column containing the type of hour (night=0, day=1)\n",
    "records_df[\"pickup_hour_type\"] = np.where(\n",
    "    (records_df[\"pickup_hour\"] < 6) | (records_df[\"pickup_hour\"] > 21), 0, 1\n",
    ")\n",
    "\n",
    "# Display the first five rows of the data frame\n",
    "records_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** using on the pickup date and time column, we created seven new variables, including the date, month of year, day of year, day of week, type of day (weekday, weekend), time of day, and peak hours. The time range used for the peak hours was determined using the daily traffic data analysis available on the [tomtom website](https://www.tomtom.com/en_gb/traffic-index/new-york-traffic/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** with the exception of the date column, seven new features have been created. The date column will be used below to merge the weather forecast and holidays data frames with the primary data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pickup date to datetime\n",
    "records_df['pickup_date'] = pd.to_datetime(records_df['pickup_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the weather forecast data to the main dataset\n",
    "records_df = pd.merge(\n",
    "    records_df,\n",
    "    weather_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"pickup_date\",\n",
    "    right_on=\"date\",\n",
    ")\n",
    "\n",
    "# Add the holidays data to the main dataset\n",
    "records_df = pd.merge(\n",
    "    records_df,\n",
    "    holidays_df,\n",
    "    how=\"left\",\n",
    "    left_on=\"pickup_date\",\n",
    "    right_on=\"holiday_date\",\n",
    ")\n",
    "\n",
    "# Drop datetime columns\n",
    "records_df.drop(columns=records_df.select_dtypes(include=[\"datetime64\"]), inplace=True)\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** after merging the weather and vacation data frame with the primary data set, the NaNs in the vacation and vacation type columns will need to be filled with zeros to indicate no vacations on those days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values from the holidays_type with 0\n",
    "records_df[\"holiday_type\"] = records_df[\"holiday_type\"].fillna(0)\n",
    "\n",
    "# Fill missing values from the holidays with 0\n",
    "records_df[\"holiday\"] = records_df[\"holiday\"].fillna(0)\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** null values have been replaced by zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the zones data, i.e. pickup_borough_id to the main dataset\n",
    "records_df = (\n",
    "    pd.merge(\n",
    "        records_df,\n",
    "        zones_df[[\"LocationID\", \"BoroughID\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"pickup_location_id\",\n",
    "        right_on=\"LocationID\",\n",
    "    )\n",
    "    .rename(columns={\"BoroughID\": \"pickup_borough_id\"})\n",
    "    .drop(\"LocationID\", axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Add the zones data, i.e. pickup_borough_id to the main dataset\n",
    "records_df = (\n",
    "    pd.merge(\n",
    "        records_df,\n",
    "        zones_df[[\"LocationID\", \"BoroughID\"]],\n",
    "        how=\"left\",\n",
    "        left_on=\"dropoff_location_id\",\n",
    "        right_on=\"LocationID\",\n",
    "    )\n",
    "    .rename(columns={\"BoroughID\": \"dropoff_borough_id\"})\n",
    "    .drop(\"LocationID\", axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** the pickup and dropoff borough columns contain null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the list of pickup location id where pickup borough was null\n",
    "records_df.loc[records_df[\"pickup_borough_id\"].isna(), \"pickup_location_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the list of dropoff location id where drop borough was null\n",
    "records_df.loc[records_df[\"dropoff_borough_id\"].isna(), \"dropoff_location_id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** These null values are due to pickup location 264 and 265, which are used as surrogates when the origin or destination of trips is unknown. we can therefore safely drop these records from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows containing NaNs\n",
    "records_df.dropna(inplace=True)\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** records with unknown origins and destinations have been removed from the dataset. Below, we will create an additional variable, to indicate whether the trip was made in the same borough. We will also bin the toll amounts into three separate categories ranging from 0 to 3, with 0 indicating no tolls and 3 indicating the highest amount. Finaly we will drop features that should not be used by our model for the prediction of trip time duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column indicating if the trip was within the same borough\n",
    "records_df[\"trip_within_borough\"] = np.where(\n",
    "    records_df[\"pickup_borough_id\"] == records_df[\"dropoff_borough_id\"], 1, 0\n",
    ")\n",
    "\n",
    "# Create a new column of binned tolls amounts\n",
    "records_df[\"toll\"] = records_df[\"tolls_amount\"].apply(\n",
    "    lambda x: 0 if x == 0 else (1 if x < 3 else 2)\n",
    ")\n",
    "# Drop other columns\n",
    "records_df.drop(\n",
    "    columns=[\"passenger_count\", \"fare_amount\", \"tolls_amount\"], inplace=True\n",
    ")\n",
    "\n",
    "# Display a quick description of the data frame\n",
    "records_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into a train and test data frame\n",
    "train_df, test_df = train_test_split(records_df, test_size=0.2, random_state=0)\n",
    "\n",
    "# Export the train data frame as a pickle file\n",
    "train_df.to_pickle(r'data/processed/train.pickle')\n",
    "\n",
    "# Export the test data frame as a pickle file\n",
    "test_df.to_pickle(r'data/processed/test.pickle')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBFy6LDJ28+i+mv+3BWFx7",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01.data",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "metadata": {
   "interpreter": {
    "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
