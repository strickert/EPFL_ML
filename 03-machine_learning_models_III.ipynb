{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tcp6PULBCqss"
   },
   "source": [
    "# 5. Capstone Project: Machine Learning Models III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxLOusgfEwks"
   },
   "source": [
    "***\n",
    "\n",
    "![headerall](./images/headers/header_all.jpg)\n",
    "\n",
    "##  Goals\n",
    "\n",
    "### Project:\n",
    "In this work, we will first analyze where and when traffic congestion is highest and lowest in New York State. We will then build different machine learning models capable of predicting cab travel times in and around New York City using only variables that can be easily obtained from a smartphone app or a website. We will then compare their performance and explore the possibility of using additional variables such as weather forecasts and holidays to improve the predictive performance of the models.\n",
    "\n",
    "### Section:\n",
    "In this section, we will use the knowledge gained during the exploratory data analysis to perform the final feature transformation. Next, we will create and compare the performance of three machine learning models based on linear regression, support vector machine, and a gradient enhanced decision tree. Hyperparameters will be optimized for each model to achieve the best possible performance.\n",
    "\n",
    "## Data\n",
    "### External Datasets:\n",
    "- Weather Forecast: The 2018 NYC weather forecast was collected from the [National Weather Service Forecast Office](https://w2.weather.gov/climate/index.php?wfo=okx) website. Daily measurements were taken from January to December 2018 in Central Park. These measures are given in imperial units and include daily minimum and maximum temperatures, precipitations, snowfall, and snow depth.\n",
    "\n",
    "- Holidays: The 2018 NYC holidays list was collected from the [Office Holiday](https://www.officeholidays.com/countries/usa/new-york/2021) website. The dataset contains the name, date, and type of holidays for New York.\n",
    "\n",
    "- Taxi Zones: The NYC Taxi Zones dataset was collected from the [NYC Open Data](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc) website. It contains the pickup and drop-off zones (Location IDs) for the Yellow, Green, and FHV Trip Records. The taxi zones are based on the NYC Department of City Planning’s Neighborhood.\n",
    "\n",
    "### Primary Datasets:\n",
    "\n",
    "- Taxi Trips: The 2018 NYC Taxi Trip dataset was collected from the [Google Big Query](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips?project=jovial-monument-300209&folder=&organizationId=) platform. The dataset contains more than 100'000'000 Yellow Taxi Trip records for 2018 and contains an extensive amount of variables including the pick-up and drop-off dates/times, pick-up and drop-off locations, trip distances, itemized fares, rate types, payment types, and driver-reported passenger counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8tvC_V2FEj5"
   },
   "source": [
    "***\n",
    "## Table of Content:\n",
    "    1. Data Preparation\n",
    "        1.1 External Datasets\n",
    "            1.1.1 Weather Forecast Dataset\n",
    "            1.1.2 Holidays Dataset\n",
    "            1.1.3 Taxi Zones Dataset\n",
    "        1.2 Primary Dataset\n",
    "            1.2.1 Taxi Trips Dataset\n",
    "            1.2.2 Taxi Trips Subset\n",
    "    2. Exploratory Data Analysis\n",
    "        2.1 Primary Dataset\n",
    "            2.1.1 Temporal Analysis\n",
    "            2.1.2 Spatio-Temporal Analysis\n",
    "        2.2 External Datasets\n",
    "            2.2.1 Temporal Analysis of Weather Data\n",
    "            2.2.2 Temporal Analysis of Holidays Data\n",
    "        2.3 Combined Dataset\n",
    "            2.3.1 Overall Features Correlation\n",
    "    3. Machine Learning Models\n",
    "        3.1 Data Preparation\n",
    "        3.2 Baselines\n",
    "        3.2 Model Training\n",
    "            3.2.1 Linear Regression\n",
    "            3.2.2 Support Vector Machine\n",
    "            3.2.3 Gradient Boosted Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1WFf7qqEi03",
    "tags": []
   },
   "source": [
    "***\n",
    "## Python Libraries and Magic commands Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python core libraries\n",
    "import time\n",
    "\n",
    "# Import data processing libraries gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import Visualization libraries\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import median_absolute_error as MAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up magic commands\n",
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_tr: (824654, 33)\n",
      "y_tr: (824654,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the train dataset\n",
    "train_df = pd.read_pickle(r'data/processed/train.pickle')\n",
    "\n",
    "# Get the independant variables from the train dataset\n",
    "X_tr = train_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the train dataset\n",
    "y_tr = train_df[\"trip_duration\"]\n",
    "\n",
    "print('X_tr:', X_tr.shape)\n",
    "print('y_tr:', y_tr.shape, y_tr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_te: (206156, 33)\n",
      "y_te: (206156,) float64\n"
     ]
    }
   ],
   "source": [
    "# Import the test dataset\n",
    "test_df = pd.read_pickle(r'data/processed/test.pickle')\n",
    "\n",
    "# Get the independant variables from the test dataset\n",
    "X_te = test_df.drop(\"trip_duration\", axis=1)\n",
    "\n",
    "# Get the dependant variable from the test dataset\n",
    "y_te = test_df[\"trip_duration\"]\n",
    "\n",
    "print('X_te:', X_te.shape)\n",
    "print('y_te:', y_te.shape, y_te.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Functions Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that performs preprocessing steps to the selected dataset\n",
    "def preprocess(data, categorical_cols, continuous_cols, transform_cols, polynome_deg=1):\n",
    "\n",
    "    # Create a copy of the data frame\n",
    "    df = data.copy()\n",
    "\n",
    "    # One-hot encode categorical features\n",
    "    df = pd.get_dummies(df, columns=categorical_cols, dummy_na=False)\n",
    "\n",
    "    # Log-transform numerical variables\n",
    "    for col in transform_cols:\n",
    "        df[col] = np.log(df[col])\n",
    "    \n",
    "    # Add polynomial features\n",
    "    for col in continuous_cols:\n",
    "        if polynome_deg > 1:\n",
    "            for poly in range(polynome_deg + 1):\n",
    "                df[\"{}**{}\".format(col, poly)] = df[col] ** poly\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Variable Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of categorical variables\n",
    "categorical_cols = [\n",
    "    \"pickup_month\",\n",
    "    \"pickup_week\",\n",
    "    \"pickup_weekday\",\n",
    "    \"pickup_weekday_type\",\n",
    "    \"pickup_hour\",\n",
    "    \"pickup_hour_type\",\n",
    "    \"wf_avg_temp_lvl\",\n",
    "    \"wf_prec_lvl\",\n",
    "    \"wf_new_snow_lvl\",\n",
    "    \"wf_snow_depth_lvl\",\n",
    "    \"holiday_type\",\n",
    "    \"holiday\",\n",
    "    \"trip_within_borough\",\n",
    "    \"tolls_amount_lvl\",\n",
    "]\n",
    "\n",
    "# Define a list of continuous variables\n",
    "continuous_cols = [\n",
    "    \"trip_distance\",\n",
    "    \"tolls_amount\",\n",
    "    \"wf_avg_temp\",\n",
    "    \"wf_prec\",\n",
    "    \"wf_new_snow\",\n",
    "    \"wf_snow_depth\",\n",
    "    \"pickup_zone_latitude\",\n",
    "    \"pickup_zone_longitude\",\n",
    "    \"pickup_borough_latitude\",\n",
    "    \"pickup_borough_longitude\",\n",
    "    \"dropoff_zone_latitude\",\n",
    "    \"dropoff_zone_longitude\",\n",
    "    \"dropoff_borough_latitude\",\n",
    "    \"dropoff_borough_longitude\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# 3. Machine Learning Models\n",
    "## 3.2 Machine Learning Models: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get id column names from the train dataset\n",
    "id_cols = [c for c in train_df.columns if \"_id\" in c]\n",
    "\n",
    "# Remove ID features in the train dataset\n",
    "X_tr.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Remove ID features in the test dataset\n",
    "X_te.drop(id_cols, axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the train dataset\n",
    "X_tr.drop(\"pickup_yearday\", axis=1, inplace=True)\n",
    "\n",
    "# Drop the pickup day of the year variable from the test dataset\n",
    "X_te.drop(\"pickup_yearday\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.2.1 Model Training: Support Vector Machine\n",
    "\n",
    "## Goals:\n",
    "\n",
    "## Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support Vector Machine: testing different training sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the support vector machine regression model is: 91\n",
      "The MAE of the support vector machine regression model model is: 5\n",
      "Exectution time: 2 sec\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr[:100], y_tr[:100])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_1 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the support vector machine regression model is: 60\n",
      "The MAE of the support vector machine regression model model is: 3\n",
      "Exectution time: 17 sec\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr[:1000], y_tr[:1000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_2 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the support vector machine regression model is: 31\n",
      "The MAE of the support vector machine regression model model is: 2\n",
      "Exectution time: 169 sec\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr[:10000], y_tr[:10000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_3 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MSE of the support vector machine regression model is: 23\n",
      "The MAE of the support vector machine regression model model is: 2\n",
      "Exectution time: 2314 sec\n"
     ]
    }
   ],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr[:100000], y_tr[:100000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time_4 = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time_4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** subsample dataset and decrease/optimise feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  Support Vector Machine: testing different feature spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday data\n",
    "X_tr_sub1 = X_tr.drop(columns=[col for col in X_tr.columns if \"holiday\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without holiday data\n",
    "X_te_sub1 = X_te.drop(columns=[col for col in X_te.columns if \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub1.shape)\n",
    "print(\"X_te:\", X_te_sub1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr_sub1[:80000], y_tr[:80000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te_sub1)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without weather forecast data\n",
    "X_tr_sub2 = X_tr.drop(columns=[col for col in X_tr.columns if \"wf\" in col])\n",
    "\n",
    "# Create a subset of the test matrix without weather forecast data\n",
    "X_te_sub2 = X_te.drop(columns=[col for col in X_te.columns if \"wf\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub2.shape)\n",
    "print(\"X_te:\", X_te_sub2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the prerocessed train dataset\n",
    "svr_model.fit(X_tr_sub2[:80000], y_tr[:80000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te_sub2)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of the train matrix without holiday and weather forecastdata\n",
    "X_tr_sub3 = X_tr.drop(columns=[col for col in X_tr.columns if \"wf\" in col or \"holiday\" in col])\n",
    "                               \n",
    "# Create a subset of the test matrix without holiday and weather forecast data\n",
    "X_te_sub3 = X_te.drop(columns=[col for col in X_te.columns if \"wf\" in col or \"holiday\" in col])\n",
    "\n",
    "print(\"X_tr:\", X_tr_sub3.shape)\n",
    "print(\"X_te:\", X_te_sub3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time at exectution start\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model.fit(X_tr_sub3[:80000], y_tr[:80000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model.predict(X_te_sub3)\n",
    "\n",
    "# Calculate execution time\n",
    "end_time = time.time() - start_time\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print('Exectution time: {:.0f} sec'.format(end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:** Improve the accuracy with which the model is able to predict for new data.\n",
    "Reduce computational cost.\n",
    "Produce a more interpretable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Support Vector Machine: testing ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 81 candidates, totalling 405 fits\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "svr_model_params = {\n",
    "    'svc__C':  np.logspace(-4, 4, num=9),\n",
    "    'svc__gamma':  np.logspace(-4, 4, num=9)\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "svr_model_gs = GridSearchCV(\n",
    "    svr_model,\n",
    "    svr_model_params,\n",
    "    cv=3,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model_gs.fit(X_tr[:100], y_tr[:100])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model_gs.predict(X_te)\n",
    "\n",
    "print('The MSE of the support vector machine regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the support vector machine regression model model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params: \\n\", svr_y_pred.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that performs standardization and fit the data to a support vector machine model\n",
    "svr_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SVR()\n",
    ")\n",
    "\n",
    "# Define a set of hyperparameters to be tested during gridsearch\n",
    "svr_model_params = {\n",
    "    'svc__kernel': ['rbf', 'linear'],\n",
    "    'svc__C':  np.logspace(-4, 4, num=9),\n",
    "    'svc__gamma':  np.logspace(-4, 4, num=9)\n",
    "}\n",
    "\n",
    "# Create a gridsearch object to find the optimum hyperparameters\n",
    "svr_model_gs = GridSearchCV(\n",
    "    svr_model,\n",
    "    svr_model_params,\n",
    "    cv=5,\n",
    "    return_train_score=True,\n",
    "    verbose=True,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# Fit and evaluate the pipeline to the preprocessed train dataset\n",
    "svr_model_gs.fit(X_tr[:10000], y_tr[:10000])\n",
    "\n",
    "# Predict the target variable of the preprocessed test dataset with the best hyperparameters \n",
    "svr_y_pred = svr_model_gs.predict(X_te)\n",
    "\n",
    "print('The MSE of the linear regression model is: {:.0f}'.format(MSE(y_te, svr_y_pred)))\n",
    "print('The MAE of the linear regression model is: {:.0f}'.format(MAE(y_te, svr_y_pred)))\n",
    "\n",
    "print(\"\\n The best parameters across all searched params:\\n\",svr_y_pred.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPBFy6LDJ28+i+mv+3BWFx7",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "01.data",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "f0bf25c6fa4e57b7fa36a98c8f4215014c209bd59bf7f61e8cb59f3c04a18758"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
